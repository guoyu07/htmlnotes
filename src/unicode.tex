\part{HTML Unicode}






\chapter{Character encodings in HTML}




HTML (Hypertext Markup Language) has been in use since 1991, but HTML 4.0 (December 1997) was the first standardized version where international characters were given reasonably complete treatment. When an HTML document includes special characters outside the range of seven-bit ASCII two goals are worth considering: the information's integrity, and universal browser display.

HTML于1991年面世，但一直要到1997年推出4.0版本以后，才对国际化这题目有一个较好的回应。在此之前，为了保证所有人都能够正常阅读内容，当要对所有用到ASCII字集以外字符 的规范。这是为了两个目的：

\begin{compactitem}
\item 保持储存在 HTML 文件内的内容的完整性，
\item 让市面上大多数浏览器都能正确显示文本的内容。
\end{compactitem}

在 head 里面加入这段内容：

\begin{lstlisting}[language=HTML]
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
\end{lstlisting}

或者将utf-8改成所使用的编码\cite{character_encoding}。





\section{Specifying the document's character encoding}


There are several ways to specify which character encoding is used in the document. First, the web server can include the character encoding or "charset" in the Hypertext Transfer Protocol (HTTP) Content-Type header, which would typically look like this:

\begin{lstlisting}[language=HTML]
Content-Type: text/html; charset=ISO-8859-1
\end{lstlisting}

This method gives the HTTP server a convenient way to alter document's encoding according to content negotiation; certain HTTP server software can do it, for example Apache with the module \texttt{mod\_charset\_lite}.

For HTML it is possible to include this information inside the head element near the top of the document:

\begin{lstlisting}[language=HTML]
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
\end{lstlisting}

HTML5 also allows the following syntax to mean exactly the same:

\begin{lstlisting}[language=HTML]
<meta charset="utf-8">
\end{lstlisting}

XHTML documents have a third option: to express the character encoding via XML declaration, as follows:

\begin{lstlisting}[language=HTML]
<?xml version="1.0" encoding="ISO-8859-1"?>
\end{lstlisting}

\texttt{<meta http-equiv="Content-Type">} may be interpreted directly by a browser, like an ordinary HTML tag,[citation needed] or it may be used by the HTTP server to generate corresponding headers when it serves the document. The HTTP/1.1 header specification for a HTML document must label an appropriate encoding in the Content-Type header, missing charset= parameter results in acceptance of ISO-8859-1 (so HTTP/1.1 formally does not offer such option as an unspecified character encoding), and this specification supersedes all HTML (or XHTML) meta element ones. This can pose a problem if the server generates an incorrect header and one does not have the access or the knowledge to change them.

As each of these methods explain to the receiver how the file being sent should be interpreted, it would be inappropriate for these declarations not to match the actual character encoding used. Because a server usually[citation needed] does not know how a document is encoded—especially if documents are created on different platforms or in different regions—many servers[citation needed] simply do not include a reference to the "charset" in the Content-Type header, thus avoiding making false promises. However, if the document does not specify the encoding either, this may result in the equally bad situation where the user agent displays mojibake because it cannot find out which character encoding was used. Due to widespread and persistent ignorance of HTTP charset= over the Internet (at its server side), WWW Consortium disappointed in HTTP/1.1’s strict approach[7] and encourages browser developers to use some fixes in violation of RFC 2616.

If a user agent reads a document with no character encoding information, it can fall back to using some other information. For example, it can rely on the user's settings, either browser-wide or specific for a given document, or it can pick a default encoding based on the user's language. For Western European languages, it is typical and fairly safe to assume Windows-1252, which is similar to ISO-8859-1 but has printable characters in place of some control codes. The consequence of choosing incorrectly is that characters outside the printable ASCII range (32 to 126) usually appear incorrectly. This presents few problems for English-speaking users, but other languages regularly—in some cases, always—require characters outside that range. In CJK environments where there are several different multi-byte encodings in use, auto-detection is also often employed. Finally, browsers usually permit the user to override incorrect charset label manually as well.

It is increasingly common for multilingual websites and websites in non-Western languages to use UTF-8, which allows use of the same encoding for all languages. UTF-16 or UTF-32, which can be used for all languages as well, are less widely used because they can be harder to handle in programming languages that assume a byte-oriented ASCII superset encoding, and they are less efficient for text with a high frequency of ASCII characters, which is usually the case for HTML documents.

Successful viewing of a page is not necessarily an indication that its encoding is specified correctly. If the page's creator and reader are both assuming some platform-specific character encoding, and the server does not send any identifying information, then the reader will nonetheless see the page as the creator intended, but other readers on different platforms or with different native languages will not see the page as intended.



\section{Character references}


In addition to native character encodings, characters can also be encoded as character references, which can be numeric character references (decimal or hexadecimal) or character entity references. Character entity references are also sometimes referred to as named entities, or HTML entities for HTML. HTML's usage of character references derives from SGML.






\subsection{HTML character references}


A numeric character reference in HTML refers to a character by its Universal Character Set/Unicode code point, and uses the format

\verb|&#nnnn;|

or 

\verb|&#xhhhh;|

where \texttt{nnnn} is the code point in decimal form, and hhhh is the code point in hexadecimal form. The x must be lowercase in XML documents. The nnnn or hhhh may be any number of digits and may include leading zeros. The hhhh may mix uppercase and lowercase, though uppercase is the usual style.

Not all web browsers or email clients used by receivers of HTML documents, or text editors used by authors of HTML documents, will be able to render all HTML characters. Most modern software is able to display most or all of the characters for the user's language, and will draw a box or other clear indicator for characters they cannot render.

For codes from 0 to 127, the original 7-bit ASCII standard set, most of these characters can be used without a character reference. Codes from 160 to 255 can all be created using character entity names. Only a few higher-numbered codes can be created using entity names, but all can be created by decimal number character reference.

Character entity references can also have the format \texttt{\&name;} where name is a case-sensitive alphanumeric string. For example, "$\lambda$" can also be encoded as \texttt{\&lambda;} in an HTML document. The character entity references \texttt{\&lt;}, \texttt{\&gt;}, \texttt{\&quot;} and \texttt{\&amp;} are predefined in HTML and SGML, because \texttt{<}, \texttt{>}, \texttt{"} and \texttt{\&} are already used to delimit markup. This notably does not include XML's \texttt{\&apos;} (') entity. 

Unnecessary use of HTML character references may significantly reduce HTML readability. If the character encoding for a web page is chosen appropriately, then HTML character references are usually only required for markup delimiting characters as mentioned above, and for a few special characters (or none at all if a native Unicode encoding like UTF-8 is used). Incorrect HTML entity escaping may also open up security vulnerabilities for injection attacks such as cross-site scripting. If HTML attributes are left unquoted, certain characters, most importantly whitespace, such as space and tab, must be escaped using entities. Other languages related to HTML have their own methods of escaping characters.






\subsection{Illegal characters}

HTML forbids[8] the use of the characters with Universal Character Set/Unicode code points

\begin{compactitem}
\item 0 to 31, except 9, 10, and 13 (C0 control characters)
\item 127 (DEL character)
\item 128 to 159 (x80 – x9F, C1 control characters)
\item 55296 to 57343 (xD800 – xDFFF, the UTF-16 surrogate halves)
\end{compactitem}

The Unicode standard also forbids:

\begin{compactitem}
\item 65534 and 65535 (xFFFE – xFFFF), non-characters, related to xFEFF, the byte order mark.
\end{compactitem}

These characters are not even allowed by reference. That is, you should not even write them as numeric character references. However, references to characters 128–159 are commonly interpreted by lenient web browsers as if they were references to the characters assigned to bytes 128–159 (decimal) in the Windows-1252 character encoding. This is in violation of HTML and SGML standards, and the characters are already assigned to higher code points, so HTML document authors should always use the higher code points. For example, for the trademark sign (™), use \texttt{\&\#8482;}, not \texttt{\&\#153;}.

The characters 9 (tab), 10 (linefeed), and 13 (carriage return) are allowed in HTML documents, but, along with 32 (space) are all considered "whitespace".[9] The "form feed" control character, which would be at 12, is not allowed in HTML documents, but is also mentioned as being one of the "white space" characters – perhaps an oversight in the specifications. In HTML, most consecutive occurrences of white space characters, except in a <pre> block, are interpreted as comprising a single "word separator" for rendering purposes. A word separator is typically rendered a single en-width space in European languages, but not in all the others.







\subsection{XML character references}

Unlike traditional HTML with its large range of character entity references, in XML there are only five predefined character entity references. These are used to escape characters that are markup sensitive in certain contexts:

\begin{compactitem}
\item \texttt{\&amp;} $\Longrightarrow$ \texttt{\&} (ampersand, U+0026)
\item \texttt{\&lt;} $\Longrightarrow$ \texttt{<} (less-than sign, U+003C)
\item \texttt{\&gt;} $\Longrightarrow$ \texttt{>} (greater-than sign, U+003E)
\item \texttt{\&quot;} $\Longrightarrow$ \texttt{"} (quotation mark, U+0022)
\item \texttt{\&apos;} $\Longrightarrow$ \texttt{'} (apostrophe, U+0027)
\end{compactitem}


All other character entity references have to be defined before they can be used. For example, use of \texttt{\&eacute;} (which gives é, Latin lower-case E with acute accent, U+00E9 in Unicode) in an XML document will generate an error unless the entity has already been defined. XML also requires that the x in hexadecimal numeric references be in lowercase: for example \texttt{\&\#xA1b} rather than \texttt{\&\#XA1b}. XHTML, which is an XML application, supports the HTML entity set, along with XML's predefined entities.

However, use of \texttt{\&apos;} in XHTML should generally be avoided for compatibility reasons. \texttt{\&\#39;} or \texttt{\&\#x0027;} may be used instead.

\texttt{\&amp;} has the special problem that it starts with the character to be escaped. A simple Internet search finds millions of sequences \texttt{\&amp;amp;amp;amp;} ... in HTML pages for which the algorithm to replace an ampersand by the corresponding character entity reference was probably applied repeatedly http://www.google.co.uk/search?q=\%22\%26amp\%3Bamp\%3Bamp\%3Bamp\%3B\%22 Missing or empty \texttt{|title= (help)}.


\chapter{Unicode and HTML}



Web pages authored using hypertext markup language (HTML) may contain multilingual text represented with the Unicode universal character set. Key to the relationship between Unicode and HTML is the relationship between the "document character set" which defines the set of characters that may be present in a HTML document and assigns numbers to them and the "external character encoding" or "charset" used to encode a given document as a sequence of bytes.

In RFC 1866, the initial HTML 2.0 standard, the document character set was defined as ISO-8859-1. It was extended to ISO 10646 (which is basically equivalent to Unicode) by RFC 2070. It does not vary between documents of different languages or created on different platforms. The external character encoding is chosen by the author of the document (or the software the author uses to create the document) and determines how the bytes used to store and/or transmit the document map to characters from the document character set. Characters not present in the chosen external character encoding may be represented by character entity references.

The relationship between Unicode\cite{unicode} and HTML tends to be a difficult topic for many computer professionals, document authors, and web users alike. The accurate representation of text in web pages from different natural languages and writing systems is complicated by the details of character encoding, markup language syntax, font, and varying levels of support by web browsers.



\section{HTML document characters}

Web pages are typically HTML or XHTML documents. Both types of documents consist, at a fundamental level, of characters, which are graphemes and grapheme-like units, independent of how they manifest in computer storage systems and networks.

An HTML document is a sequence of Unicode characters. More specifically, HTML 4.0 documents are required to consist of characters in the HTML document character set : a character repertoire wherein each character is assigned a unique, non-negative integer code point. This set is defined in the HTML 4.0 DTD, which also establishes the syntax (allowable sequences of characters) that can produce a valid HTML document. The HTML document character set for HTML 4.0 consists of most, but not all, of the characters jointly defined by Unicode and ISO/IEC 10646: the Universal Character Set (UCS).

Like HTML documents, an XHTML document is a sequence of Unicode characters. However, an XHTML document is an XML document, which, while not having an explicit "document character" layer of abstraction, nevertheless relies upon a similar definition of permissible characters that cover most, but not all, of the Unicode/UCS character definitions. The sets used by HTML and XHTML/XML are slightly different, but these differences have little effect on the average document author.

Regardless of whether the document is HTML or XHTML, when stored on a file system or transmitted over a network, the document's characters are encoded as a sequence of bit octets (bytes) according to a particular character encoding. This encoding may either be a Unicode Transformation Format, like UTF-8, that can directly encode any Unicode character, or a legacy encoding, like Windows-1252, that cannot. However, even when using encodings that do not support all Unicode characters, the encoded document may make use of numeric character references. For example \texttt{\&\#x263A;} (☺) is used to indicate a smiling face character in the Unicode character set.


\subsection{Character encoding}

In order to support all Unicode characters without resorting to numeric character references, a web page must have an encoding covering all of Unicode. The most popular is UTF-8, where the ASCII characters, such as English letters, digits, and some other common characters are preserved unchanged against ASCII. This makes HTML code (such as <br> and </div>) unchanged compared to ASCII. Characters outside the ASCII range are stored in 2-4 bytes. It is also possible to use UTF-16 where most characters are stored as two bytes with varying endianness, which is supported by modern browsers but less commonly used.




\subsection{Numeric character references}

In order to work around the limitations of legacy encodings, HTML is designed such that it is possible to represent characters from the whole of Unicode inside an HTML document by using a numeric character reference: a sequence of characters that explicitly spell out the Unicode code point of the character being represented. A character reference takes the form \texttt{\&\#N;}, where N is either a decimal number for the Unicode code point, or a hexadecimal number, in which case it must be prefixed by x. The characters that compose the numeric character reference are universally representable in every encoding approved for use on the Internet.


For example, a Unicode code point like U+5408, which corresponds to a particular Chinese character, has to be converted to a decimal number, preceded by \texttt{\&\#} and followed by ;, like this: \texttt{\&\#21512;}, which produces this: 合 (if it doesn't look like a Chinese character, see Template:Special characters).


The support for hexadecimal in this context is more recent, so older browsers might have problems displaying characters referenced with hexadecimal numbers—but they will probably have a problem displaying Unicode characters above code point 255 anyway. To ensure better compatibility with older browsers, it is still a common practice to convert the hexadecimal code point into a decimal value (for example \texttt{\&\#21512;} instead of \texttt{\&\#x5408;}).



\subsection{Named character entities}

In HTML, there is a standard set of 252 named character entities for characters - some common, some obscure - that are either not found in certain character encodings or are markup sensitive in some contexts (for example angle brackets and quotation marks). Although any Unicode character can be referenced by its numeric code point, some HTML document authors prefer to use these named entities instead, where possible, as they are less cryptic and were better supported by early browsers.

Character entities can be included in an HTML document via the use of entity references, which take the form \texttt{\&EntityName;}, where EntityName is the name of the entity. For example, \texttt{\&mdash;}, much like \texttt{\&\#8212;} or \texttt{\&\#x2014;}, represents U+2014: the em dash character "—" even if the character encoding used doesn't contain that character.





\section{Character encoding determination}


In order to correctly process HTML, a web browser must ascertain which Unicode characters are represented by the encoded form of an HTML document. In order to do this, the web browser must know what encoding was used.





\subsection{Encoding information}

When a document is transmitted via a MIME message or a transport that uses MIME content types such as an HTTP response, the message may signal the encoding via a Content-Type header, such as Content-Type: text/html; charset=UTF-8. Other external means of declaring encoding are permitted but rarely used. If the document uses an Unicode encoding, the encoding info might also be present in the form of a Byte order mark. Finally, the encoding can be declared via the HTML syntax. For the text/html serialisation then, as long as the page is encoded in an extension of ASCII (such as UTF-8, and thus, not if the page is using UTF-16), a meta element, like \texttt{<meta http-equiv="content-type" content="text/html; charset=UTF-8">} or (starting with HTML5) \texttt{<meta charset="UTF-8">} can be used. For HTML pages serialized as XML, then declaration options is to either rely on the encoding default (which for XML documents is UTF-8), or to use an XML encoding declaration. The meta attribute plays no role in HTML served as XML.




\subsection{Encoding defaults}

An encoding default applies when there is no external or internal encoding declaration and also no Byte order mark. While the encoding default for HTML pages served as XML is required to be UTF-8, the encoding default for a regular Web page (that is: for HTML pages serialized as \texttt{text/html}) varies depending on the localization of the browser. For a system set up mainly for Western European languages, it will generally be Windows-1252. For the Russian locale, the default is typically Windows-1251. For a browser from a location where legacy multi-byte character encodings are prevalent, some form of auto-detection is likely to be applied.




\subsection{Encoding trends}

Because of the legacy of 8-bit text representations in programming languages and operating systems and the desire to avoid burdening users with the need to understand the nuances of encoding, many text editors used by HTML authors are unable or unwilling to offer a choice of encodings when saving files to disk and often do not even allow input of characters beyond a very limited range. Consequently many HTML authors are unaware of encoding issues and may not have any idea what encoding their documents actually use. Misunderstandings, such as the belief that the encoding declaration affects a change in the actual encoding (whereas it is actually just a label that could be inaccurate), is also a reason for this editor attitude. Another factor contributing in the same direction, is the arrival of UTF-8 — which greatly diminishes the need for other encodings, and thus modern editors tends to default, as recommended by the HTML5 specification, to UTF-8.







\subsection{Byte order mark/Unicode sniffing}

For both serializations of HTML (content-type "\texttt{text/html}" and content/type "\texttt{application/xhtml+xml}"), the Byte order mark (BOM) is an effective way to transmit encoding information within an HTML document. For UTF-8, the BOM is optional, while it is a must for the UTF-16 and the UTF-32 encodings. (Note: UTF-16 and UTF-32 without the BOM are formally known under different names, they are different encodings, and thus needs some form of encoding declaration – see UTF-16BE, UTF-16LE, UTF-32LE and UTF-32BE.) The use of the BOM character (U+FEFF) means that the encoding automatically declares itself to any processing application. Processing applications need only look for an initial 0x0000FEFF, 0xFEFF or 0xEFBBBF in the byte stream to identify the document as UTF-32, UTF-16 or UTF-8 encoded respectively. No additional metadata mechanisms are required for these encodings since the byte-order mark includes all of the information necessary for processing applications. In most circumstances the byte-order mark character is handled by editing applications separately from the other characters so there is little risk of an author removing or otherwise changing the byte order mark to indicate the wrong encoding (as can happen when the encoding is declared in English/Latin script). If the document lacks a byte-order mark, the fact that the first non-blank printable character in an HTML document is supposed to be "\texttt{<}" (U+003C) can be used to determine a UTF-8/UTF-16/UTF-32 encoding.





\subsection{Encoding overriding}

Many HTML documents are served with inaccurate encoding information, or no encoding information at all. In order to determine the encoding in such cases, many browsers allow the user to manually select an encoding name from a list. They may also employ an encoding auto-detection algorithm that works in concert with or — in the case of the BOM and in case of HTML served as XML — against the manual override.

For HTML documents which are \texttt{text/html} serialized, manual override may apply to all documents, or only those for which the encoding cannot be ascertained by looking at declarations and/or byte patterns. The fact that the manual override is present and widely used hinders the adoption of accurate encoding declarations on the Web; therefore the problem is likely to persist. But note that Internet Explorer, Chrome and Safari — for both XML and text/html serializations — do not permit the encoding to be overridden whenever the page includes the BOM.

For HTML documents serialized with the preferred XML label — \texttt{application/xhtml+xml}, manual encoding override is not permitted. To override the encoding of such an XML document would mean that that the document stopped being XML, as it is a fatal error for XML documents to have an encoding declaration with detectable errors. Currently, Gecko browsers such as Firefox, abide to this rule, whereas the bulk of the other common browsers that support HTML as XML, such as Webkit browsers (Chrome/Safari) do allow the encoding of XHTML documents to be manually overridden.




\section{Web browser support}

Many browsers are only capable of displaying a small subset of the full Unicode repertoire. Here is how your browser displays various Unicode code points\footnote{To display all of the characters above, you may need to install one or more large multilingual fonts, like \href{http://en.wikipedia.org/wiki/Code2000}{Code2000}.}:

\zihao{6}

\begin{longtable}{|l|l|l|l|}
%%
\multicolumn{4}{r}{}
\tabularnewline\hline
Character	&HTML char ref				&Unicode name	&browser displays
\endhead
%%


%%
\caption{Web browser support}\\
\hline
Character	&HTML char ref				&Unicode name	&browser displays
\endhead
%%

%%
\multicolumn{4}{r}{}
\endfoot
%%

%%
\endlastfoot
%%
\hline
U+0041	&\texttt{\&\#65;} or \texttt{\&\#x41;}	&Latin capital letter A	&A\\
\hline
U+00DF	&\texttt{\&\#223;} or \texttt{\&\#xDF;}	&Latin small letter Sharp S	&ß\\
\hline
U+00FE	&\texttt{\&\#254;} or \texttt{\&\#xFE;}	&Latin small letter Thorn	&þ\\
\hline
U+0394	&\texttt{\&\#916;} or \texttt{\&\#x394;}	&Greek capital letter Delta	&Δ\\
\hline
U+017D	&\texttt{\&\#381;} or \texttt{\&\#x17D;}	&Latin capital letter Z with caron (used in Central Europe)	&Ž\\
\hline
U+0419	&\texttt{\&\#1049;} or \texttt{\&\#x419;}	&Cyrillic capital letter Short I	&Й\\
\hline
U+05E7	&\texttt{\&\#1511;} or \texttt{\&\#x5E7;}	&Hebrew letter Qof&	\\
\hline
U+0645	&\texttt{\&\#1605;} or \texttt{\&\#x645;}	&Arabic letter Meem &\\
\hline
U+0E57	&\texttt{\&\#3671;} or \texttt{\&\#xE57;}	&Thai digit 7	&๗\\
\hline
U+1250	&\texttt{\&\#4688;} or \texttt{\&\#x1250;}	&Ge'ez syllable Qha	&ቐ\\
\hline
U+3042	&\texttt{\&\#12354;} or \texttt{\&\#x3042;}	&Hiragana letter A (Japanese)	&あ\\
\hline
U+53F6	&\texttt{\&\#21494;} or \texttt{\&\#x53F6;}	&CJK Unified Ideograph-53F6 (Simplified Chinese "Leaf")	&叶\\
\hline
U+8449	&\texttt{\&\#33865;} or \texttt{\&\#x8449;}	&CJK Unified Ideograph-8449 (Traditional Chinese "Leaf")	&葉\\
\hline
U+B5AB	&\texttt{\&\#46507;} or \texttt{\&\#xB5AB;}	&Hangul syllable Tteolp (Korean "Ssangtikeut Eo Rieulbieup")	&떫\\
\hline
U+16A0	&\texttt{\&\#5792;} or \texttt{\&\#x16A0;}	&Runic letter Fehu	&ᚠ\\
\hline
U+0D37	&\texttt{\&\#3383;} or \texttt{\&\#x0D37;}	&Malayalam letter ഷ (ṣa)	&ഷ\\
\hline
\end{longtable}

\zihao{5}

Some web browsers, such as Mozilla Firefox, Opera, Safari and Internet Explorer (from version 7 on), are able to display multilingual web pages by intelligently choosing a font to display each individual character on the page. They will correctly display any mix of Unicode blocks, as long as appropriate fonts are present in the operating system.

Older browsers, such as Netscape Navigator 4.77 and Internet Explorer 6, can only display text supported by the current font associated with the character encoding of the page, and may misinterpret numeric character references as being references to code values within the current character encoding, rather than references to Unicode code points. When you are using such a browser, it is unlikely that your computer has all of those fonts, or that the browser can use all available fonts on the same page. As a result, the browser will not display the text in the examples above correctly, though it may display a subset of them. Because they are encoded according to the standard, though, they will display correctly on any system that is compliant and does have the characters available. Further, those characters given names for use in named entity references are likely to be more commonly available than others.

For displaying characters outside the Basic Multilingual Plane, like the Gothic letter faihu, which is variant of runic letter Fehu in the table above, some systems (like Windows 2000) need manual adjustments of their settings.



\section{Frequency of usage}


According to internal data from Google's web index, in December 2007 the UTF-8 Unicode encoding became the most frequently used encoding on web pages, overtaking both ASCII (US) and 8859-1/1252 (Western European).





\chapter{Language code}

A language code\cite{language_code} is a code that assigns letters and/or numbers as identifiers or classifiers for languages. These codes may be used to organize library collections or presentations of data, to choose the correct localizations and translations in computing, and as a shorthand designation for longer forms of language-name.







\section{Difficulties of classification}


Language code schemes attempt to classify within the complex world of human languages, dialects, and variants. Most schemes make some compromises between being general and being complete enough to support specific dialects.

For example, most people in Central America and South America speak Spanish. Spanish spoken in Mexico will be slightly different from Spanish spoken in Peru. Different regions of Mexico will have slightly different dialects and accents of Spanish. A language code scheme might group these all as "Spanish" for choosing a keyboard layout, most as "Spanish" for general usage, or separate each dialect to allow region-specific idioms.


\section{Common schemes}

Some common language code schemes include:

\begin{landscape}

\zihao{6}

\begin{longtable}{|p{80pt}|p{200pt}|p{140pt}|p{140pt}|}
%head
\multicolumn{4}{r}{}
\tabularnewline\hline
\multirow{2}{50pt}{Scheme}	&\multirow{2}{180pt}{Notes}	&\multicolumn{2}{c|}{Examples}	\\ \cline{3-4}
								&								&Codes for English	& Codes for Spanish
\endhead
%endhead

%firsthead
\hline
\multirow{2}{50pt}{Scheme}	&\multirow{2}{180pt}{Notes}	&\multicolumn{2}{c|}{Examples}	\\ \cline{3-4}
								&								&Codes for English	& Codes for Spanish
\endfirsthead
%endfirsthead

%foot
\multicolumn{4}{r}{}
\endfoot
%endfoot

%lastfoot
\endlastfoot
%endlastfoot
\hline
ISO 639	&The original ISO standard from 1967 to 2002. 
			 Now obsolete, it was replaced by ISO 639‑1, 
			 ISO 639‑2, and ISO 639‑3. Sometimes used as 
			 a shorthand for the union of all 639 standard codes.	& eng – three-letter code 
																	\newline enm – Middle English, c. 1100–1500 
																	\newline ang – Old English, c. 450–1100 
																	\newline cpe – other English-based creoles and pidgins 
																	\newline EN – English or American two-letter capital code	& esl – three-letter code 
																															\newline pa – alternative three-letter code 
																															\newline ES – Spanish two-letter capital code\\
\hline
ISO 639‑1	&Two-letter code system made official in 2002, 
			 containing 136 codes. Many systems use two-letter 
			 ISO 639‑1 codes supplemented by three-letter ISO 
			 639‑2 codes when no two-letter code is applicable.	&en															&es – Spanish\\
\hline
ISO 639‑2	&Three-letter system of 464 codes.					&eng – three-letter code 
																\newline enm – Middle English, c. 1100–1500
																\newline ang – Old English, c. 450–1100
																\newline cpe – other English-based creoles and pidgins		&spa – Spanish\\
\hline
ISO 639‑3	&An extension of ISO 639‑2 to cover all known,
			 living or dead, \newline spoken or written languages in 7,589 entries.	&eng – three-letter code 
																		\newline enm – Middle English, c. 1100–1500 
																		\newline aig – Antigua and Barbuda Creole English 
																		\newline ang – Old English, c. 450–1100 
																		\newline svc – Vincentian Creole English 
																		\newline others										&spa – Spanish 
																															\newline spq – Spanish, Loreto-Ucayali 
																															\newline ssp – Spanish sign language 
																															\newline others\\
\hline
Old SIL codes&	Codes created for use in the Ethnologue, 
				 a publication of SIL International that lists 
				 language statistics. The publication now uses 
				 ISO 639‑3 codes.	 									&													&SPN\\
\hline
IETF language tag&	An IETF best practice, currently specified 
				 by RFC 5646 and RFC 4647, for language tags
				 easy to parse by computer. The tag system is 
				 extensible to region, dialect, and private designations.		&en – English, as shortest ISO 639 code.
																		\newline en-US – English as used in the United States\footnote{US is the ISO 3166‑1 country code for the United States.}&es – Spanish, as shortest ISO 639 code.			\newline es-419 – Spanish appropriate for the Latin America and Caribbean region, using the UN M.49 region code\\
\hline
LS‑2010		&Two-digit + one to six letter Linguasphere 
				 code system published in 2000, updated 2010, 
				 containing over 32,000 codes.							&(within hierarchy of Linguasphere-2010 codes, as follows:)
																		\newline 5= Indo-European phylosector
																		\newline 52= Germanic phylozone
																		\newline 52-A Germanic set
																		\newline 52-AB English + Anglo-Creole chain
																		\newline 52-ABA English
																			\newline \indent  net
																		\newline 52-ABA-c
																			\newline \indent Global English
																			\newline \indent outer unit
																		\newline 52-ABA-ca to 52-ABA-cwe					&(within hierarchy of Linguasphere-2010 codes, as follows:)
																															\newline  5= Indo-European phylosector
																															\newline  51= Romanic phylozone
																															\newline 51-A Romance set
																															\newline  51-AA Romance chain
																															\newline 51-AAA West Romance net
																															\newline 51-AAA-b Español/Castellano outer unit
																															\newline 51-AAA-ba to 51-AAA-bkk\\
\hline
Verbix Language Codes& Constructed codes starting with old SIL codes and adding more \newline information.& &\\
\hline
\end{longtable}

\end{landscape}



\zihao{5}









\clearpage
\bibliographystyle{plainnat}
\bibliography{htmlnotes}






































